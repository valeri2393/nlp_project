{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemiy/miniforge-pypy3/envs/nlp_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "\n",
    "# Ð”Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐµÐ¹ Ð½Ð°Ð³Ð»ÑÐ´Ð½Ð¾ÑÑ‚Ð¸ Ð±ÑƒÐ´ÐµÐ¼ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ Ñ€ÑƒÑÑÐºÐ¾ÑÐ·Ñ‹Ñ‡Ð½Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸ÐµÐ¹ GPT Ð¾Ñ‚ Ð¡Ð±ÐµÑ€Ð°.\n",
    "# ÐÐ¸Ð¶Ðµ Ð½Ð°Ñ…Ð¾Ð´ÑÑ‚ÑÑ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¸ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°.\n",
    "model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"ÐœÐ¾Ñ€Ð¾Ð· Ð¸ ÑÐ¾Ð»Ð½Ñ†Ðµ; Ð´ÐµÐ½ÑŒ Ñ‡ÑƒÐ´ÐµÑÐ½Ñ‹Ð¹!\n",
    "Ð•Ñ‰Ðµ Ñ‚Ñ‹ Ð´Ñ€ÐµÐ¼Ð»ÐµÑˆÑŒ, Ð´Ñ€ÑƒÐ³ Ð¿Ñ€ÐµÐ»ÐµÑÑ‚Ð½Ñ‹Ð¹ â€”\n",
    "ÐŸÐ¾Ñ€Ð°, ÐºÑ€Ð°ÑÐ°Ð²Ð¸Ñ†Ð°, Ð¿Ñ€Ð¾ÑÐ½Ð¸ÑÑŒ:\n",
    "ÐžÑ‚ÐºÑ€Ð¾Ð¹ ÑÐ¾Ð¼ÐºÐ½ÑƒÑ‚Ñ‹ Ð½ÐµÐ³Ð¾Ð¹ Ð²Ð·Ð¾Ñ€Ñ‹\n",
    "ÐÐ°Ð²ÑÑ‚Ñ€ÐµÑ‡Ñƒ ÑÐµÐ²ÐµÑ€Ð½Ð¾Ð¹ ÐÐ²Ñ€Ð¾Ñ€Ñ‹,\n",
    "Ð—Ð²ÐµÐ·Ð´Ð¾ÑŽ ÑÐµÐ²ÐµÑ€Ð° ÑÐ²Ð¸ÑÑŒ!\n",
    "Ð’ÐµÑ‡Ð¾Ñ€, Ñ‚Ñ‹ Ð¿Ð¾Ð¼Ð½Ð¸ÑˆÑŒ, Ð²ÑŒÑŽÐ³Ð° Ð·Ð»Ð¸Ð»Ð°ÑÑŒ,\n",
    "ÐÐ° Ð¼ÑƒÑ‚Ð½Ð¾Ð¼ Ð½ÐµÐ±Ðµ Ð¼Ð³Ð»Ð° Ð½Ð¾ÑÐ¸Ð»Ð°ÑÑŒ;\n",
    "Ð›ÑƒÐ½Ð°, ÐºÐ°Ðº Ð±Ð»ÐµÐ´Ð½Ð¾Ðµ Ð¿ÑÑ‚Ð½Ð¾,\n",
    "Ð¡ÐºÐ²Ð¾Ð·ÑŒ Ñ‚ÑƒÑ‡Ð¸ Ð¼Ñ€Ð°Ñ‡Ð½Ñ‹Ðµ Ð¶ÐµÐ»Ñ‚ÐµÐ»Ð°,\n",
    "Ð˜ Ñ‚Ñ‹ Ð¿ÐµÑ‡Ð°Ð»ÑŒÐ½Ð°Ñ ÑÐ¸Ð´ÐµÐ»Ð° â€”\n",
    "Ð Ð½Ñ‹Ð½Ñ‡Ðµ... Ð¿Ð¾Ð³Ð»ÑÐ´Ð¸ Ð² Ð¾ÐºÐ½Ð¾:\n",
    "ÐŸÐ¾Ð´ Ð³Ð¾Ð»ÑƒÐ±Ñ‹Ð¼Ð¸ Ð½ÐµÐ±ÐµÑÐ°Ð¼Ð¸\n",
    "Ð’ÐµÐ»Ð¸ÐºÐ¾Ð»ÐµÐ¿Ð½Ñ‹Ð¼Ð¸ ÐºÐ¾Ð²Ñ€Ð°Ð¼Ð¸,\n",
    "Ð‘Ð»ÐµÑÑ‚Ñ Ð½Ð° ÑÐ¾Ð»Ð½Ñ†Ðµ, ÑÐ½ÐµÐ³ Ð»ÐµÐ¶Ð¸Ñ‚;\n",
    "ÐŸÑ€Ð¾Ð·Ñ€Ð°Ñ‡Ð½Ñ‹Ð¹ Ð»ÐµÑ Ð¾Ð´Ð¸Ð½ Ñ‡ÐµÑ€Ð½ÐµÐµÑ‚,\n",
    "Ð˜ ÐµÐ»ÑŒ ÑÐºÐ²Ð¾Ð·ÑŒ Ð¸Ð½ÐµÐ¹ Ð·ÐµÐ»ÐµÐ½ÐµÐµÑ‚,\n",
    "Ð˜ Ñ€ÐµÑ‡ÐºÐ° Ð¿Ð¾Ð´Ð¾ Ð»ÑŒÐ´Ð¾Ð¼ Ð±Ð»ÐµÑÑ‚Ð¸Ñ‚.\n",
    "Ð’ÑÑ ÐºÐ¾Ð¼Ð½Ð°Ñ‚Ð° ÑÐ½Ñ‚Ð°Ñ€Ð½Ñ‹Ð¼ Ð±Ð»ÐµÑÐºÐ¾Ð¼\n",
    "ÐžÐ·Ð°Ñ€ÐµÐ½Ð°. Ð’ÐµÑÐµÐ»Ñ‹Ð¼ Ñ‚Ñ€ÐµÑÐºÐ¾Ð¼\n",
    "Ð¢Ñ€ÐµÑ‰Ð¸Ñ‚ Ð·Ð°Ñ‚Ð¾Ð¿Ð»ÐµÐ½Ð½Ð°Ñ Ð¿ÐµÑ‡ÑŒ.\n",
    "ÐŸÑ€Ð¸ÑÑ‚Ð½Ð¾ Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ Ñƒ Ð»ÐµÐ¶Ð°Ð½ÐºÐ¸.\n",
    "ÐÐ¾ Ð·Ð½Ð°ÐµÑˆÑŒ: Ð½Ðµ Ð²ÐµÐ»ÐµÑ‚ÑŒ Ð»Ð¸ Ð² ÑÐ°Ð½ÐºÐ¸\n",
    "ÐšÐ¾Ð±Ñ‹Ð»ÐºÑƒ Ð±ÑƒÑ€ÑƒÑŽ Ð·Ð°Ð¿Ñ€ÐµÑ‡ÑŒ?\n",
    "Ð¡ÐºÐ¾Ð»ÑŒÐ·Ñ Ð¿Ð¾ ÑƒÑ‚Ñ€ÐµÐ½Ð½ÐµÐ¼Ñƒ ÑÐ½ÐµÐ³Ñƒ,\n",
    "Ð”Ñ€ÑƒÐ³ Ð¼Ð¸Ð»Ñ‹Ð¹, Ð¿Ñ€ÐµÐ´Ð°Ð´Ð¸Ð¼ÑÑ Ð±ÐµÐ³Ñƒ\n",
    "ÐÐµÑ‚ÐµÑ€Ð¿ÐµÐ»Ð¸Ð²Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ\n",
    "Ð˜ Ð½Ð°Ð²ÐµÑÑ‚Ð¸Ð¼ Ð¿Ð¾Ð»Ñ Ð¿ÑƒÑÑ‚Ñ‹Ðµ,\n",
    "Ð›ÐµÑÐ°, Ð½ÐµÐ´Ð°Ð²Ð½Ð¾ ÑÑ‚Ð¾Ð»ÑŒ Ð³ÑƒÑÑ‚Ñ‹Ðµ,\n",
    "Ð˜ Ð±ÐµÑ€ÐµÐ³, Ð¼Ð¸Ð»Ñ‹Ð¹ Ð´Ð»Ñ Ð¼ÐµÐ½Ñ.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemiy/miniforge-pypy3/envs/nlp_project/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ð¼ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² .txt Ñ„Ð°Ð¹Ð»\n",
    "train_path = \"train_dataset.txt\"\n",
    "with open(train_path, \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=64)\n",
    "\n",
    "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð°Ñ‚Ð°Ð»Ð¾Ð´ÐµÑ€Ð° (Ð½Ð°Ñ€ÐµÐ·Ð°ÐµÑ‚ Ñ‚ÐµÐºÑÑ‚ Ð½Ð° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾ Ð´Ð»Ð¸Ð½Ðµ ÐºÑƒÑÐºÐ¸)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./finetuned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The output directory\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# overwrite the content of the output directory\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of training epochs\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch size for training\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch size for evaluation\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of warmup steps for learning rate scheduler\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# to make \"virtual\" batch size larger\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     ),  \u001b[38;5;66;03m# Optimizer and lr scheduler\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m<string>:128\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge-pypy3/envs/nlp_project/lib/python3.11/site-packages/transformers/training_args.py:1641\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1636\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1641\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal_than_2_3)\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1652\u001b[0m     )\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1665\u001b[0m ):\n",
      "File \u001b[0;32m~/miniforge-pypy3/envs/nlp_project/lib/python3.11/site-packages/transformers/training_args.py:2149\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/miniforge-pypy3/envs/nlp_project/lib/python3.11/site-packages/transformers/utils/generic.py:59\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/miniforge-pypy3/envs/nlp_project/lib/python3.11/site-packages/transformers/training_args.py:2055\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2055\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2056\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2058\u001b[0m         )\n\u001b[1;32m   2059\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned\",  # The output directory\n",
    "    overwrite_output_dir=True,  # overwrite the content of the output directory\n",
    "    num_train_epochs=200,  # number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    warmup_steps=10,  # number of warmup steps for learning rate scheduler\n",
    "    gradient_accumulation_steps=16,  # to make \"virtual\" batch size larger\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    optimizers=(\n",
    "        torch.optim.AdamW(model.parameters(), lr=1e-5),\n",
    "        None,\n",
    "    ),  # Optimizer and lr scheduler\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
