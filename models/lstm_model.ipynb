{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id  general food interior service  \\\n",
      "0          0        0   10       10      10   \n",
      "1          1        0    9       10       9   \n",
      "2          2        0    9       10      10   \n",
      "3          3        0    -        5      10   \n",
      "4          4        0    7       10      10   \n",
      "\n",
      "                                                text  \n",
      "0  Вытянули меня сегодня в город и раз уж была в ...  \n",
      "1  проводили корпоратив на 60 чел. в этот - уже т...  \n",
      "2  Был в Гостях с женой один раз и еще раз с жено...  \n",
      "3  Бар понравился на первый взгляд .  Интерьер к ...  \n",
      "4  В « Bel Canto » мы отмечали юбилей моего отца ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Замените 'path_to_file.jsonl' на путь к вашему файлу\n",
    "file_path = '/Users/valeriaalesnikova/Desktop/bootcamp/nlp_project-1/restaurants_reviews.jsonl'\n",
    "\n",
    "# Чтение файла jsonl в DataFrame\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Печать первых 5 строк для проверки\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Токенизация\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "# Преобразование текстов в последовательности\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=100)\n",
    "\n",
    "# Подготовка меток\n",
    "labels = pd.get_dummies(df['general']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 72ms/step - accuracy: 0.9147 - loss: 0.4013 - val_accuracy: 0.8347 - val_loss: 0.4229\n",
      "Epoch 2/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 76ms/step - accuracy: 0.9230 - loss: 0.2561 - val_accuracy: 0.9612 - val_loss: 0.1613\n",
      "Epoch 3/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 88ms/step - accuracy: 0.9428 - loss: 0.1750 - val_accuracy: 0.9015 - val_loss: 0.2380\n",
      "Epoch 4/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 73ms/step - accuracy: 0.9596 - loss: 0.1223 - val_accuracy: 0.8720 - val_loss: 0.3038\n",
      "Epoch 5/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 75ms/step - accuracy: 0.9752 - loss: 0.0774 - val_accuracy: 0.8350 - val_loss: 0.4862\n",
      "Epoch 6/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 76ms/step - accuracy: 0.9860 - loss: 0.0457 - val_accuracy: 0.8573 - val_loss: 0.4943\n",
      "Epoch 7/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 80ms/step - accuracy: 0.9942 - loss: 0.0220 - val_accuracy: 0.8623 - val_loss: 0.5525\n",
      "Epoch 8/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 82ms/step - accuracy: 0.9970 - loss: 0.0122 - val_accuracy: 0.8236 - val_loss: 0.7534\n",
      "Epoch 9/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 82ms/step - accuracy: 0.9986 - loss: 0.0058 - val_accuracy: 0.8736 - val_loss: 0.5871\n",
      "Epoch 10/10\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 77ms/step - accuracy: 0.9981 - loss: 0.0072 - val_accuracy: 0.8107 - val_loss: 1.0360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3084b8310>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Bidirectional, Layer\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[-1],),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.u = self.add_weight(name='context_vector', shape=(input_shape[-1], 1),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        u_t = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        a_t = tf.nn.softmax(tf.tensordot(u_t, self.u, axes=1), axis=1)\n",
    "        output = x * a_t\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "# Определение архитектуры модели\n",
    "input_layer = Input(shape=(100,))\n",
    "embedding_layer = Embedding(10000, 128)(input_layer)\n",
    "lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
    "attention_layer = AttentionLayer()(lstm_layer)\n",
    "output_layer = Dense(labels.shape[1], activation='softmax')(attention_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(padded_sequences, labels, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка модели на тестовых данных (при наличии)\n",
    "loss, accuracy = model.evaluate(padded_sequences, labels)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для предсказания класса отзыва\n",
    "def predict_review(review):\n",
    "    seq = tokenizer.texts_to_sequences([review])\n",
    "    padded = pad_sequences(seq, padding='post', maxlen=100)\n",
    "    pred = model.predict(padded)\n",
    "    return df['general'].unique()[pred.argmax()]\n",
    "\n",
    "# Пример предсказания\n",
    "review = \"The food was great and the service was excellent.\"\n",
    "print(f'Predicted class: {predict_review(review)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "model.save('restaurant_review_model.h5')\n",
    "\n",
    "# Загрузка модели\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('restaurant_review_model.h5', custom_objects={'AttentionLayer': AttentionLayer})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
